<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h2 id="project-proposal">Project Proposal</h2>
<h1 id="machine-translation-under-atypical-use.">Machine translation under atypical use.</h1>
<p>Machine translation is the process of converting a text from a source language to target language by the use of machine learning algorithms The inherent complexity of language means that having a rule based approach to solve the problem would be tedious and inefficient, hence the need for machine learning based algorithms. Neural Machine learning methods have found use in modelling the ambiguity and flexibility of h human language, but they lack predictability and are trained on corpora based on 'formal use' and perform poorly on text 'in the wild'. Hence in this project we will try to build methods/models that are trained on standard corpora but also perform well on distributionally shfited database i.e. text with the use of profanities, punctuation errors, grammatical mistakes, slangs etc.</p>
<h1 id="dataset">Dataset</h1>
<p>For the training we will use standard dataset like the WMT’20 English-Russian corpus, English-Russian Newstest’19 and the corpus of news data collected from GlobalVoices News service. The text in these is mostly used formally. To check our evaluation on shifted dataset we will use Reddit corpus prepared for the WMT’19 robustness challenge. These dataset is annotated by expert human translators and supplied by the shifts challenge team (NeurIPS 2021). This dataset is tagged with the following anomalies:- Punctuation anomalies, Capitalisation anomalies, Fluency anomalies, Slang anomalies, Emoji anomalies and Tags anomalies. The method of evaluation for the robustness and uncertainty in the prediction will be through area under error - retention curves.</p>
<h1 id="previous-methods-and-baselines.">Previous methods and baselines.</h1>
<p>For the baseline we use an ensemble of 3 transformers-big models trained on the WMT'20 En-Ru Corpus. Some of the previous of robust models have been done of small scale image classification problems but not on multi - modal problems like translation where multiple correct sentences are possible for one input sentence. Such approaches have been extended to structures prediction tasks like ours. They can be characterised by Ensemble and sampling based, Prior networks and temperature scaling.</p>
<h1 id="our-approach-and-schedule.">Our approach and Schedule.</h1>
<p>Upto midway: Do statistical analysis on the dataset, try existing ensemble based methods and existing techniques on robust learning. After midway: Try to include ELMO word embeddings, implement a paper on max-min robust optimisation, assessment and optimisation of the models. Our objective will be to improve baseline performance, try to come with additional measure for uncertainity determination and get to the evaluation leaderboards.</p>
<h1 id="relevant-papers">Relevant Papers</h1>
<ol style="list-style-type: decimal">
<li>Wang, Y., Wu, L., Xia, Y., Qin, T., Zhai, C., &amp; Liu, T.-Y. (2020). Transductive Ensemble Learning for Neural Machine Translation. Proceedings of the AAAI Conference on Artificial Intelligence, 34(04), 6291-6298.</li>
<li>Paul Michel, Tatsunori B. Hashimoto, &amp; Graham Neubig (2021). Modeling the Second Player in Distributionally Robust Optimization. ArXiv, abs/2103.10282.</li>
<li>Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., &amp; Zettlemoyer, L. (2018). Deep contextualized word representations. In Proc. of NAACL.</li>
<li>Malinin, A., Band, N., Ganshin, Chesnokov, G., Gal, Gales, M., Noskov, A., Ploskonosov, A., Prokhorenkova, L., Provilkov, I., Raina, V., Raina, V., Roginskiy, D., Shmatova, M., Tigar, P., &amp; Yangel, B. (2021). Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks. arXiv preprint arXiv:2107.07455.</li>
</ol>

<h2 id="project-midway">Project Midway</h2>
<h1 id="summary-of-relevant-papers">Summary of Relevant Papers</h1>
<ol>
<li><p><strong>Malinin, A., Band, N., Ganshin, Chesnokov, G., Gal, Gales, M., Noskov, A., Ploskonosov, A., Prokhorenkova, L., Provilkov, I., Raina, V., Raina, V., Roginskiy, D., Shmatova, M., Tigar, P., &amp; Yangel, B. (2021). Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks. arXiv preprint arXiv:2107.07455.</strong>
This paper describes the task and the dataset that must used for training and validation. It is generally assumed that the training data and deployment data come from identical distribution, but for many tasks and datasets in the wild this may not be the case and the deployment data is heavily shifted or out of domain. We want our datasets to be robust to such shifts and also to give reliable uncertainty estimates on its predictions. This estimates must be predictable and reliable so that we know when to trust the model. The task we will be focusing on is Neural Machine Translation(NMT). This is a structured prediction task. For the in-Domain dataset we use WMT’20 English-Russian corpus,English-Russian Newstest’19, GlobalVoices News service which will be used for training, development, and evaluation respectively. They contain roughly 70 million, 2000 and 3000 sentences respectively. For shifted dataset we use, WMT’19 MTNT Reddit and Shifts reddit prepared for this challenge. They total about 3,500 sentences that will be used for development and evaluation. Since this dataset is crawled from reddit. Assessment of models must account for both its performance on shifted dataset and also reliability of its uncertainty measures. For performance on evaluation set we can use standard metrics like BLEU and GleU. For uncertainty estimates we can assess its ability to distinguish in-domain and out-of-domain input using area under receiver operator curve or precision recall (AUROC/ AUPR) but this is not much useful models may be robust some examples in the distributed dataset and give low uncertainty for these examples. To jointly assess performance and uncertainty area under error retention curve is suggested. For the baseline models 3-transformer big models are trained using fairseq. These models are currently best for many NLP tasks and use self attention based RNNs. For uncertainty estimates they use a method described in a referenced paper which we will discuss below.</p>
</li>
<li><p><strong>Malinin, A., &amp; Gales, M. (2020). Uncertainty Estimation in Autoregressive Structured Prediction. arXiv preprint arXiv:2002.07650.</strong>
This paper discusses predictive uncertainty estimates in Autoregressive Structured Prediction task.
Since Machine Translation is autoregressive in nature i.e. the probability of the next output word is dependent on the previous occurring words, this paper is of relevance to us. The authors decompose total uncertainty into one of data uncertainty and knowledge uncertainty. Data uncertainty is the intrinsic uncertainty associated with the data while the knowledge uncertainty is the uncertainty associated with the model due to its inability to understand the data. We are particularly interested in making knowledge uncertainty estimates. For this the authors propose using an ensemble of models trained with different initialisations. Next a predictive posterior probability is calculated by taking expectation over all the ensembles. Next the entropy for this posterior is calculated and written in terms of data and knowledge uncertainty. Apart from this two other measures expected pairwise KL-divergence and reverse mutual information are introduced to capture model diversity and their properties explored. Since these measures become intractable very soon as vocabulary sizes increase methods for their approximations are also discussed.</p>
</li>
</ol>
<h1 id="progress">Progress</h1>
<p>The training dataset turned out to be very big (around 40gb), this made working locally difficult. So the data was mounted to Drive and  Google Colab was used to process it. Even though scripts to train the baseline transformer model were provided preprocessing them turned to be time taking. The data had to be broken into subwords tokens and apply byte pair encoding for it to be in the correct format. Beam search was used to make inference.</p>
</body>
</html>
